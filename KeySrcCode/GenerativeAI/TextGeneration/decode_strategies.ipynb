{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Decode Strategy of Text Generation\n",
    "\n",
    "GPT等自回归模型的解码策略会直接影响大模型的输出效果。在回归预测第$N$个token时，模型基于前序$N-1$个tokens计算第$N$个token的条件概率分布$P(w_N | w_1, \\cdots, w_{N-1})$。\n",
    "解码策略基于预测分布来决定模型的第$N$个token输出。常见的解码策略包含：\n",
    "* Greedy Search\n",
    "* Beam Search\n",
    "* Sample\n",
    "* Beam Sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# gpt2 has no PAD token\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T12:43:26.138412Z",
     "end_time": "2023-05-09T12:47:54.201909Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Greedy Search\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "<img src=\"./Figure/DecoderDemo.png\" width=512>\n",
    "</div>\n",
    "\n",
    "基于贪心搜索的解码策略，每步都选择概率最大的token：\n",
    "$$ \\mathrm{argmin}_{w_N} P(w_N | w_1, \\cdots, w_{N-1}) $$\n",
    "\n",
    "缺点：\n",
    "* 可能错过全局概率最大的序列，如上图所示，The dog has的总概率最大。\n",
    "* 由于缺少随机性，模型在输出一个重复token后，可能陷入重复输出序列的循环。\n",
    "* 基于贪心搜索的解码与模型训练的目标函数类似，容易复述训练数据，缺乏创造性。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Greedy Search的代码执行逻辑"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "<img src=\"./Figure/GreedySearch.png\" width=1280>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Greedy Search 快速上手案例"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_prompt = \"This sunday we may\"\n",
    "in_ids = tokenizer(in_prompt, return_tensors='pt').input_ids\n",
    "\n",
    "logit_processor = LogitsProcessorList(\n",
    "    [\n",
    "        MinLengthLogitsProcessor(10, eos_token_id=model.generation_config.eos_token_id),\n",
    "        RepetitionPenaltyLogitsProcessor(1.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
    "\n",
    "# 路径： $install_path/transformers/generation/utils.py\n",
    "outputs = model.greedy_search(\n",
    "    in_ids, logits_processor=logit_processor, stopping_criteria=stopping_criteria\n",
    ")\n",
    "\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T12:49:58.474188Z",
     "end_time": "2023-05-09T12:49:59.289391Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Beam Search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Beam Search的核心思想是，保留当前最佳的$n$个序列，并针对每个序列都再计算最后的$n$个next token，然后从$n\\times n$个结果中，保留$n$个概率乘积最大的序列。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Beam Search的代码执行逻辑"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "<img src=\"./Figure/GreedySearch.png\" width=1280>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Beam Search 快速上手案例"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    BeamSearchScorer\n",
    ")\n",
    "\n",
    "tokenizer_beam = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model_beam = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "encoder_in_str = \"translate English to German: That is good.\"\n",
    "encoder_in_ids = tokenizer_beam(encoder_in_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "num_beams = 3\n",
    "in_ids = torch.ones((num_beams, 1), device=model_beam.device, dtype=torch.long)\n",
    "in_ids = in_ids * model_beam.config.decoder_start_token_id\n",
    "\n",
    "model_kwargs = {\n",
    "    \"encoder_outputs\": model_beam.get_encoder()(\n",
    "        encoder_in_ids.repeat_interleave(num_beams, dim=0),\n",
    "        return_dict=True\n",
    "    )\n",
    "}\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size=1,\n",
    "    num_beams=num_beams,\n",
    "    num_beam_hyps_to_keep=2,\n",
    "    device=model_beam.device,\n",
    ")\n",
    "\n",
    "logit_processor = LogitsProcessorList(\n",
    "    [\n",
    "        NoRepeatNGramLogitsProcessor(2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "outputs = model_beam.beam_search(\n",
    "    in_ids, beam_scorer, logits_processor=logit_processor, **model_kwargs\n",
    ")\n",
    "\n",
    "result = tokenizer_beam.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T15:09:51.347868Z",
     "end_time": "2023-05-09T15:09:52.243719Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "随机采样策略根据第$N$个token的预测概率来进行采样。\n",
    "为保证生成的语句是通顺的，这里会引入temperature来改变预测概率分布，使其偏向于更高概率的结果，具体做法如下，在softmax概率中引入$t\\in (0, 1]$，通过调整$t$的大小，可以避免从长尾分布中采样出不通顺的结果。\n",
    "$$ P( x | x_{1:N-1}) = \\frac{e^{u_t / t}}{\\sum_{t^{\\prime}} e^{u_{t^{\\prime}/t}}}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Top-k sampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "<img src=\"./Figure/TopKSampling.png\" width=1024>\n",
    "</div>\n",
    "\n",
    "Top-k采样只保留概率最高的$k$个token，然后计算重新归一化后的概率分布用以采样。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Top-p (Nucleus) sampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "<img src=\"Figure/TopPSampling.png\" width=1028>\n",
    "</div>\n",
    "\n",
    "Top-p采样保留累积概率大于等于$p$的tokens，再重新计算归一化后的概率分布用以采样。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sample的代码执行逻辑"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./Figure/Sample.png\" width=1280>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Today is a beautiful day, and we're all so glad that our country is finally recognizing that the world isn't always a better place than it was five decades\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper\n",
    ")\n",
    "\n",
    "# 同上\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "in_prompt = \"Today is a beautiful day, and\"\n",
    "in_ids = tokenizer(in_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logit_processor = LogitsProcessorList(\n",
    "    [\n",
    "        MinLengthLogitsProcessor(15, eos_token_id=model.generation_config.eos_token_id),\n",
    "    ]\n",
    ")\n",
    "\n",
    "logit_warper = LogitsProcessorList(\n",
    "    [\n",
    "        TopKLogitsWarper(50),\n",
    "        TopPLogitsWarper(0.9),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [\n",
    "        MaxLengthCriteria(max_length=32)\n",
    "    ]\n",
    ")\n",
    "\n",
    "outputs = model.sample(\n",
    "    in_ids,\n",
    "    logits_processor=logit_processor,\n",
    "    logits_warper=logit_warper,\n",
    "    stopping_criteria=stopping_criteria,\n",
    ")\n",
    "\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Beam Sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Beam Sample对Sample的结果进行排序，并保留当前最佳的$n$个序列，这样既保证了多样性和创造性，又可避免不通顺的输出。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Beam Sample的代码执行逻辑"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./Figure/BeamSample.png\" width=1280>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wie alt bist du?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TemperatureLogitsWarper\n",
    ")\n",
    "\n",
    "encoder_in_str = \"translate English into German: How old are you?\"\n",
    "encoder_in_ids = tokenizer_beam(encoder_in_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "num_beams = 3\n",
    "in_ids = torch.ones((num_beams, 1), device=model_beam.device, dtype=torch.long)\n",
    "in_ids = in_ids * model_beam.config.decoder_start_token_id\n",
    "\n",
    "model_kwargs = {\n",
    "    \"encoder_outputs\": model_beam.get_encoder()(\n",
    "        encoder_in_ids.repeat_interleave(num_beams, dim=0),\n",
    "        return_dict=True,\n",
    "    )\n",
    "}\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size=1,\n",
    "    max_length=model_beam.config.max_length,\n",
    "    num_beams=num_beams,\n",
    "    device=model_beam.device,\n",
    ")\n",
    "\n",
    "logit_warper = LogitsProcessorList(\n",
    "    [\n",
    "        TemperatureLogitsWarper(0.8),\n",
    "        TopKLogitsWarper(50),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [\n",
    "        MaxLengthCriteria(max_length=32)\n",
    "    ]\n",
    ")\n",
    "\n",
    "outputs = model_beam.beam_sample(\n",
    "    in_ids,\n",
    "    beam_scorer,\n",
    "    logits_warper=logit_warper,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "result = tokenizer_beam.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T17:21:05.932038Z",
     "end_time": "2023-05-09T17:21:06.818523Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
