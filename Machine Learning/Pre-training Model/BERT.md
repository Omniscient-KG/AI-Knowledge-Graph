## BERT

### 0. 概括

文章提出一种预训练双向Transformer模型
【Bidirectional Encoder Representations from Transformers, BERT】。
BERT是一种自监督的语言模型，通过利用大量未标注的语料库提供输入，可以预测句子中每个词汇的上下文。
它改进了单向语言模型，增加了双向概率，并采用先进的预训练技术，从而获得了比以往更好的性能。
此外，BERT模型可以应用于多个NLU任务，包括语义相似性和问答，并在相应的基准数据集上取得了最先进的结果。




### 1. 方法

>#### *模型架构*


![alt "Model Structure Comparison"](./Figure/Bert%20Structure.png)



> #### *预训练任务*
> 





### 2. 评估